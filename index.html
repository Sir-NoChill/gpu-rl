<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>CMPUT 605 - GPU Programming for Continuous Learning</title>
    <style>
        body {
            font-family: Georgia, serif;
            max-width: 900px;
            margin: 0 auto;
            padding: 20px;
            line-height: 1.6;
            color: #333;
            background: #fff;
        }

        h1 {
            color: #000;
            border-bottom: 2px solid #333;
            padding-bottom: 10px;
            margin-bottom: 20px;
            font-size: 1.8em;
        }

        h2 {
            color: #000;
            margin-top: 30px;
            margin-bottom: 15px;
            font-size: 1.3em;
            border-bottom: 1px solid #ccc;
            padding-bottom: 5px;
        }

        h3 {
            color: #333;
            margin-top: 20px;
            margin-bottom: 10px;
            font-size: 1.1em;
            font-weight: bold;
        }

        .header-info {
            background: #f5f5f5;
            padding: 15px;
            margin-bottom: 20px;
            border-left: 3px solid #666;
        }

        .header-info p {
            margin: 5px 0;
        }

        .course-structure {
            margin: 20px 0;
        }

        .course-structure ol {
            margin-left: 20px;
        }

        .course-structure li {
            margin: 10px 0;
        }

        .nested-list {
            margin-top: 10px;
            margin-left: 20px;
            list-style-type: lower-roman;
        }

        .timeline {
            background: #f9f9f9;
            padding: 15px;
            margin: 20px 0;
            border-radius: 5px;
        }

        .grading-table {
            width: 100%;
            margin: 20px 0;
            border-collapse: collapse;
        }

        .grading-table th, .grading-table td {
            padding: 10px;
            text-align: left;
            border: 1px solid #ddd;
        }

        .grading-table th {
            background: #f0f0f0;
            font-weight: bold;
        }

        .grading-table tr:hover {
            background: #f5f5f5;
        }

        a {
            color: #0066cc;
            text-decoration: none;
        }

        a:hover {
            text-decoration: underline;
        }

        .date-revised {
            font-style: italic;
            color: #666;
            margin-bottom: 15px;
        }

        .emphasis {
            font-weight: bold;
            color: #000;
        }

        .section {
            margin: 30px 0;
        }

        ul {
            margin-left: 20px;
        }

        li {
            margin: 8px 0;
        }

        .project-desc {
            background: #fafafa;
            padding: 15px;
            border: 1px solid #e0e0e0;
            margin: 15px 0;
        }
    </style>
</head>
<body>
    <div class="date-revised">Revised January 2, 2026</div>

    <h1>CMPUT 605: GPU Programming for Sparse and Adaptive Networks</h1>

    <div class="header-info">
        <p><strong>TERM:</strong> Winter 2026</p>
        <p><strong>TIME:</strong> TBD - two 1:20 minute sessions per week</p>
        <p><strong>INSTRUCTOR:</strong> TBD</p>
        <p><strong>LOCATION:</strong> TBD</p>
    </div>

    <h2>Calendar Description</h2>
    <p>
        Independent study course focusing on the use of GPGPU techniques to advance sparse neural networks,
        evolving network topologies, and streaming learning by leveraging modern GPGPU hardware and developing
        compiler transformations to further leverage existing infrastructure for these learning paradigms.
    </p>

    <h2>Course Description and Goals</h2>
    <p>
        This course provides an in-depth exploration of GPU programming techniques and their application to
        sparse neural networks, adaptive network structures, and streaming learning problems. Students will gain
        hands-on experience with CUDA, HIP, and potentially OpenXLA/SPIRV, while developing expertise in GPU
        profiling, optimization, and multi-GPU scaling. The course covers the underlying architecture of modern
        GPUs, current advances in GPU hardware design, and compiler-based approaches to optimization. We are
        specifically interested in learning how GPUs can be used to accelerate sparse neural networks, evolving
        topology, and streaming learning. The course bridges theoretical understanding with practical implementation
        through collaborative assignments and a substantial research project.
    </p>

    <p>
        By the end of this course, students will be able to:
    </p>
    <ul>
        <li>Write and optimize GPU kernels using CUDA and HIP</li>
        <li>Understand modern GPU architecture and recent hardware advances</li>
        <li>Profile GPU applications and identify performance bottlenecks using profiling tools and compiler-based instrumentation</li>
        <li>Debug GPU code using modern profiling tools</li>
        <li>Apply compiler transformations to address common GPU programming issues</li>
        <li>Analyze tradeoffs between different algorithmic approaches for GPU execution</li>
        <li>Scale implementations to multi-GPU systems</li>
        <li>Design and optimize kernels for non-standard computational patterns</li>
    </ul>

    <p>
        <strong>Learning Objectives by Student Background:</strong>
    </p>
    <ul>
        <li><strong>For Deep Learning Students:</strong> Develop the ability to implement and optimize custom GPU kernels for novel neural network architectures, understand hardware constraints that shape algorithm design, and bridge the gap between algorithmic innovation and efficient execution.</li>
        <li><strong>For Computer Architecture Students:</strong> Gain practical experience with GPU programming models, understand how compiler transformations map high-level code to GPU hardware, and explore the design space of GPU architectures through performance analysis and optimization.</li>
    </ul>

    <h2>Course Structure</h2>
    <div class="course-structure">
        <ol>
            <li>
                <span class="emphasis">The Ins and Outs of CUDA/HIP</span>
                <ol class="nested-list">
                    <li>Implementation of common data structures and algorithms using CUDA, HIP and potentially OpenXLA/SPIRV</li>
                    <li>GPU profiling tools for debugging algorithms and data structures</li>
                    <li>Understanding profiling tool internals and compiler-based instrumentation techniques</li>
                    <li>Compiler transformations for addressing common GPU programming challenges (memory coalescing, bank conflicts, divergence)</li>
                    <li>Scaling algorithms and tools to multi-GPU systems/clusters</li>
                </ol>
            </li>
            <li>
                <span class="emphasis">Sparsity, Dynamic Networks, and Hardware-Underexplored Approaches</span>
                <ol class="nested-list">
                    <li>Review of research in sparse neural networks and dynamic connectivity</li>
                    <li>Understanding what has been done in evolving network structures</li>
                    <li>Analysis of streaming learning and sequential data challenges</li>
                    <li>How data movement patterns and code transformations enable efficient sparsity in hardware</li>
                    <li>Multi-GPU communication and partitioning strategies for sparse and dynamic workloads</li>
                </ol>
            </li>
            <li>
                <span class="emphasis">Course Project</span>
                <ol class="nested-list">
                    <li>Development of a course project throughout the term</li>
                    <li>Production of a comprehensive writeup for evaluation</li>
                </ol>
            </li>
        </ol>
    </div>

    <h2>Course Timeline</h2>
    <div class="timeline">
        <h3>Phase 1 & 2 (January 2026 - Mid-February)</h3>
        <p>
            Overlapping coverage of GPU programming fundamentals and literature review:
        </p>
        <ul>
            <li><strong>Day 1 (weekly):</strong> Discussion of the week's research papers</li>
            <li><strong>Day 2 (weekly):</strong> Algorithm implementation progress and technical discussions</li>
        </ul>

        <h3>Phase 3 (Mid-February - April 2026)</h3>
        <p>
            Dedicated time for course project development, with bi-weekly progress reviews and implementation refinement.
        </p>
    </div>

    <h2>Grading Scheme</h2>

    <div class="section">
        <h3>Assignments (3 Common Algorithms/Concepts) - 30%</h3>
        <p>
            Common days for implementing basic algorithms. Some discussion around implementing and other fun stuff.
            Progressively building a deeper understanding of what GPUs are. DS and algorithms course (look this up) -
            leet GPU stuff.
        </p>

        <h3>Paper Reviews - 30%</h3>
        <p>
            Structured discussion of papers we know (1 paper a week). If we finish our basics, move to 2 papers per week.
        </p>

        <h3>Project Proposal - Due February 15</h3>
        <p>
            Plan for the rest of the term. Something that bridges gaps in continual learning/RL/etc.
        </p>

        <h3>Project Deadline (April) - 40%</h3>
    </div>


    <h2>Potential Assignments</h2>

    <div class="section">
        <h3>1. Parallel Reduction</h3>
        <p>
            You will implement a naive reduction and progressively optimize it with shared memory, loop unrolling,
            and warp-shuffle techniques. Using Nsight Compute and the ROCm Profiler, you will profile each version to analyze warp stalls,
            occupancy, register usage, and memory throughput. You will write a short profiling summary explaining
            which optimizations improved performance and why, using specific metrics as evidence.
        </p>
        <p>
            This assignment should provide an opportunity to work with sparse data on GPUs, with emphasis on
            underlying architecture aids for handling control flow.
        </p>

        <h3>2. Prefix Scan + Stream Compaction</h3>
        <p>
            You will implement an efficient parallel prefix-sum (Blelloch scan) in shared memory and use it to
            build a full stream-compaction pipeline on the GPU. This assignment teaches memory coalescing,
            bank-conflict avoidance, and hierarchical parallelism. You will implement both exclusive and inclusive
            scans, then use them to compact an array (remove zeros or inactives) using scatter. This directly maps
            to building and maintaining sparse activation vectors and CSR row pointers.
        </p>

        <h3>3. Sparse Matrix-Vector Multiply (CSR + COO SpMV)</h3>
        <p>
            You will implement both COO and CSR SpMV kernels and compare their performance, memory access patterns,
            divergence behavior, and atomic usage. This includes designing different thread decomposition strategies
            (thread-per-row, warp-per-row, thread-per-nonzero). You will also implement a basic COO→CSR conversion
            using prefix sums. This assignment provides a basis for implementing sparse neural network
            forward/backward passes.
        </p>

        <h3>4. Persistent Kernels + Global Work Queues</h3>
        <p>
            You will build a persistent-threads kernel that stays resident on the GPU and continuously pulls work
            from a global queue implemented with atomics. This teaches you how to avoid kernel launch overhead and
            handle asynchronous or streaming workloads. You'll implement task distribution (pop), task creation
            (push), and termination detection safely under concurrency. This assignment mirrors what real
            sparse/dynamic NN frameworks must do to process many tiny updates in real time. We could also optionally
            use this as another chance to get more profiling experience.
        </p>
    </div>

    <h2>Other Possible Assignments</h2>

    <div class="section">
        <h3>Dynamic Graph Update + Periodic CSR Rebuild</h3>
        <p>
            You implement a dynamic adjacency structure where edges can be appended atomically (COO-style), then
            periodically rebuild a CSR representation using prefix scans and compaction. This teaches how to manage
            variable-length lists, amortize dynamic updates, and maintain consistency between representations. You
            will design a chunked, GPU-friendly memory structure for insertions, then build a fast, parallel CSR
            rebuild pipeline. This assignment is the bridge between static sparse operators and true
            evolving-connectivity neural networks.
        </p>

        <h3>Parallel BFS / Frontier Expansion</h3>
        <p>
            You will implement a GPU BFS that maintains a frontier queue and processes only active nodes on each
            iteration. This teaches workload compaction, atomic insertion into a next-level frontier, and handling
            irregular per-node degrees. You will optimize your BFS using warp-cooperative neighbor processing and
            compaction to eliminate empty work. This assignment provides deep intuition about divergence, load
            balance, and irregular memory access patterns—core challenges in sparse and dynamic neural network
            architectures.
        </p>
    </div>

    <h2>Potential Reading List</h2>

    <div class="section">
        <h3>GPU Related Papers</h3>
        <ul>
            <li>Parallel Processing (2nd edition textbook)</li>
            <li><a href="https://arxiv.org/abs/2507.10789v1">The Blackwell Architecture</a> - A microbenchmarking test on the Blackwell platform to discover architectural characteristics. <a href="https://dl.acm.org/doi/10.1145/3725843.3756041">This paper</a> does a similar thing but for the CUDA cores proper. This could make a useful comparison to the <a href="https://www.amd.com/content/dam/amd/en/documents/instinct-tech-docs/instruction-set-architectures/amd-instinct-mi300-cdna3-instruction-set-architecture.pdf">MI300A docs</a> and CDNA architecture reference to see what's up.</li>
        </ul>

        <h3>ML Related Papers</h3>
        <ul>
            <li><a href="https://openreview.net/pdf?id=Sv7DazuCn8">The Big World Hypothesis</a> - This is short and worth reading before the class to motivate this direction of work.</li>
            <li><a href="https://arxiv.org/abs/2410.14606">Streaming Reinforcement Learning Finally Works</a> - Understand what is needed to make streaming algorithms work in RL (step-size adaptation, normalization, etc.).</li>
            <li><a href="https://arxiv.org/pdf/2102.00554">Sparsity in Deep Learning: Pruning and Growth for Efficient Inference and Training In Neural Networks</a> - I have never read this paper, but at a skim it looks great. The paper is long, but the goal with this is to understand why sparsity is important and what the main algorithms people use. Even just going through the first two sections of this paper would be worth it.</li>
            <li>I want a paper that demonstrates the importance of network architecture, and how that influences performance, learning efficiency, and generalization. I haven't found one yet, but maybe we can just discuss this idea. The sparsity paper I added may actually suffice for this.</li>
            <li>The last type of paper I want to look at is a paper that learns the structure of a network online, and discusses why this is important. I don't have a paper in mind, but right now I'm considering <a href="https://arxiv.org/pdf/1607.01097">AdaNet</a>, <a href="https://link.springer.com/chapter/10.1007/978-3-032-05461-6_13">Efficient Architecture Search for Continual Learning</a>, and Neuron-Level Architecture Search for Efficient Model Design.</li>
        </ul>

        <p>
            <em>Note:</em> Continuous learning ideas might be a much better focus for paper reading in the course,
            implementing the system on an actual GPU would be an easier engineering problem and I could dig up
            papers as required to back it up.
        </p>
    </div>

    <h2>Course Topics Overview</h2>

    <div class="section">
        <h3>CUDA/HIP Programming Systems</h3>
        <p><strong>Phase 1:</strong> A library of common functions/tools/utilities + GPGPU profiling tools + Multi-GPU versions</p>
        <ul>
            <li>Implementing common utilities</li>
            <li>Implementing common algorithms</li>
            <li>Fundamentals</li>
            <li>Memory systems</li>
            <li>Finding papers? Let's look at some</li>
            <li>Open XLA backend? SPIRV? JAX?</li>
        </ul>

        <h3>Sparse Networks Context</h3>
        <ul>
            <li>In ML scaling → bigger is not better</li>
            <li>Scaling doesn't make sense in the current paradigm</li>
            <li>Squeezing as much as possible out of data</li>
            <li>Learning the network structure (hardware does not suit it)</li>
            <li>Build in the biases that make it effective for some problems</li>
            <li>Build in the specific capture techniques</li>
            <li>Current methods don't <em>really</em> scale</li>
        </ul>
    </div>
    <footer style="margin-top: 50px; padding-top: 20px; border-top: 1px solid #ccc; color: #666; font-size: 0.9em;">

        <p>Course webpage maintained by the Department of Computing Science, University of Alberta</p>
        <p>For questions or concerns, please contact the course instructor.</p>
    </footer>
</body>
</html>
