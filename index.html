<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>CMPUT 605 - GPU Programming for Continuous Learning</title>
    <style>
        body {
            font-family: Georgia, serif;
            max-width: 900px;
            margin: 0 auto;
            padding: 20px;
            line-height: 1.6;
            color: #333;
            background: #fff;
        }

        h1 {
            color: #000;
            border-bottom: 2px solid #333;
            padding-bottom: 10px;
            margin-bottom: 20px;
            font-size: 1.8em;
        }

        h2 {
            color: #000;
            margin-top: 30px;
            margin-bottom: 15px;
            font-size: 1.3em;
            border-bottom: 1px solid #ccc;
            padding-bottom: 5px;
        }

        h3 {
            color: #333;
            margin-top: 20px;
            margin-bottom: 10px;
            font-size: 1.1em;
            font-weight: bold;
        }

        .header-info {
            background: #f5f5f5;
            padding: 15px;
            margin-bottom: 20px;
            border-left: 3px solid #666;
        }

        .header-info p {
            margin: 5px 0;
        }

        .course-structure {
            margin: 20px 0;
        }

        .course-structure ol {
            margin-left: 20px;
        }

        .course-structure li {
            margin: 10px 0;
        }

        .nested-list {
            margin-top: 10px;
            margin-left: 20px;
            list-style-type: lower-roman;
        }

        .timeline {
            background: #f9f9f9;
            padding: 15px;
            margin: 20px 0;
            border-radius: 5px;
        }

        .grading-table {
            width: 100%;
            margin: 20px 0;
            border-collapse: collapse;
        }

        .grading-table th, .grading-table td {
            padding: 10px;
            text-align: left;
            border: 1px solid #ddd;
        }

        .grading-table th {
            background: #f0f0f0;
            font-weight: bold;
        }

        .grading-table tr:hover {
            background: #f5f5f5;
        }

        a {
            color: #0066cc;
            text-decoration: none;
        }

        a:hover {
            text-decoration: underline;
        }

        .date-revised {
            font-style: italic;
            color: #666;
            margin-bottom: 15px;
        }

        .emphasis {
            font-weight: bold;
            color: #000;
        }

        .section {
            margin: 30px 0;
        }

        ul {
            margin-left: 20px;
        }

        li {
            margin: 8px 0;
        }

        .project-desc {
            background: #fafafa;
            padding: 15px;
            border: 1px solid #e0e0e0;
            margin: 15px 0;
        }
    </style>
</head>
<body>
    <div class="date-revised">Revised January 2, 2026</div>

    <h1>CMPUT 605: GPU Programming for Sparse and Adaptive Networks</h1>

    <div class="header-info">
        <p><strong>TERM:</strong> Winter 2026</p>
        <p><strong>TIME:</strong> TBD - two 1:20 minute sessions per week</p>
        <p><strong>INSTRUCTOR:</strong> TBD</p>
        <p><strong>LOCATION:</strong> TBD</p>
    </div>

    <h2>Calendar Description</h2>
    <p>
        Independent study course focusing on the use of GPGPU techniques to advance sparse neural networks,
        evolving network topologies, and streaming learning by leveraging modern GPGPU hardware and developing
        compiler transformations to further leverage existing infrastructure for these learning paradigms.
    </p>

    <h2>Course Description and Goals</h2>
    <p>
        This course provides an in-depth exploration of GPU programming techniques and their application to
        sparse neural networks, adaptive network structures, and streaming learning problems. Students will gain
        hands-on experience with CUDA, HIP, and potentially OpenXLA/SPIRV, while developing expertise in GPU
        profiling, optimization, and multi-GPU scaling. The course covers the underlying architecture of modern
        GPUs, current advances in GPU hardware design, and compiler-based approaches to optimization. We are
        specifically interested in learning how GPUs can be used to accelerate sparse neural networks, evolving
        topology, and streaming learning. The course bridges theoretical understanding with practical implementation
        through collaborative assignments and a substantial research project.
    </p>

    <p>
        By the end of this course, students will be able to:
    </p>
    <ul>
        <li>Write and optimize GPU kernels using CUDA and HIP</li>
        <li>Understand modern GPU architecture and recent hardware advances</li>
        <li>Profile GPU applications and identify performance bottlenecks using profiling tools and compiler-based instrumentation</li>
        <li>Debug GPU code using modern profiling tools</li>
        <li>Apply compiler transformations to address common GPU programming issues</li>
        <li>Analyze tradeoffs between different algorithmic approaches for GPU execution</li>
        <li>Scale implementations to multi-GPU systems</li>
        <li>Design and optimize kernels for non-standard computational patterns</li>
    </ul>

    <p>
        <strong>Learning Objectives by Student Background:</strong>
    </p>
    <ul>
        <li><strong>For Deep Learning Students:</strong> Develop the ability to implement and optimize custom GPU kernels for novel neural network architectures, understand hardware constraints that shape algorithm design, and bridge the gap between algorithmic innovation and efficient execution.</li>
        <li><strong>For Computer Architecture Students:</strong> Gain practical experience with GPU programming models, understand how compiler transformations map high-level code to GPU hardware, and explore the design space of GPU architectures through performance analysis and optimization.</li>
    </ul>

    <h2>Course Structure</h2>
    <div class="course-structure">
        <ol>
            <li>
                <span class="emphasis">The Ins and Outs of CUDA/HIP</span>
                <ol class="nested-list">
                    <li>Implementation of common data structures and algorithms using CUDA, HIP and potentially OpenXLA/SPIRV</li>
                    <li>GPU profiling tools for debugging algorithms and data structures</li>
                    <li>Understanding profiling tool internals and compiler-based instrumentation techniques</li>
                    <li>Compiler transformations for addressing common GPU programming challenges (memory coalescing, bank conflicts, divergence)</li>
                    <li>Scaling algorithms and tools to multi-GPU systems/clusters</li>
                </ol>
            </li>
            <li>
                <span class="emphasis">Sparsity, Dynamic Networks, and Hardware-Underexplored Approaches</span>
                <ol class="nested-list">
                    <li>Review of research in sparse neural networks and dynamic connectivity</li>
                    <li>Understanding what has been done in evolving network structures</li>
                    <li>Analysis of streaming learning and sequential data challenges</li>
                    <li>How data movement patterns and code transformations enable efficient sparsity in hardware</li>
                    <li>Multi-GPU communication and partitioning strategies for sparse and dynamic workloads</li>
                </ol>
            </li>
            <li>
                <span class="emphasis">Course Project</span>
                <ol class="nested-list">
                    <li>Development of a course project throughout the term</li>
                    <li>Production of a comprehensive writeup for evaluation</li>
                </ol>
            </li>
        </ol>
    </div>

    <h2>Course Timeline</h2>
    <div class="timeline">
        <h3>Phase 1 & 2 (January 2026 - Mid-February)</h3>
        <p>
            Overlapping coverage of GPU programming fundamentals and literature review:
        </p>
        <ul>
            <li><strong>Day 1 (weekly):</strong> Discussion of the week's research papers</li>
            <li><strong>Day 2 (weekly):</strong> Algorithm implementation progress and technical discussions</li>
        </ul>

        <h3>Phase 3 (Mid-February - April 2026)</h3>
        <p>
            Dedicated time for course project development, with bi-weekly progress reviews and implementation refinement.
        </p>
    </div>

    <h2>Grading Scheme</h2>

    <table class="grading-table">
        <thead>
            <tr>
                <th>Component</th>
                <th>Description</th>
                <th>Weight</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>GPU Programming Assignments</td>
                <td>Three collaborative assignments implementing fundamental GPU algorithms and data structures. Progressive exploration of GPU programming concepts including profiling, optimization, and performance analysis.</td>
                <td>30%</td>
            </tr>
            <tr>
                <td>Paper Reviews & Discussions</td>
                <td>Structured weekly discussions of research papers (1 paper per week, potentially increasing to 2 papers per week as the course progresses). Active participation in class discussions.</td>
                <td>30%</td>
            </tr>
            <tr>
                <td>Course Project</td>
                <td>Research project bridging gaps in continual learning, reinforcement learning, or GPU-accelerated sparse algorithms. <strong>Project proposal due February 15.</strong> Final project due in April.</td>
                <td>40%</td>
            </tr>
        </tbody>
    </table>


    <h2>Potential Assignments</h2>

    <div class="section">
        <h3>1. Parallel Reduction</h3>
        <p>
            You will implement a naive reduction and progressively optimize it with shared memory, loop unrolling,
            and warp-shuffle techniques. Using Nsight Compute and the ROCm Profiler, you will profile each version to analyze warp stalls,
            occupancy, register usage, and memory throughput. You will write a short profiling summary explaining
            which optimizations improved performance and why, using specific metrics as evidence.
        </p>
        <p>
            This assignment should provide an opportunity to work with sparse data on GPUs, with emphasis on
            underlying architecture aids for handling control flow.
        </p>

        <h3>2. Prefix Scan + Stream Compaction</h3>
        <p>
            You will implement an efficient parallel prefix-sum (Blelloch scan) in shared memory and use it to
            build a full stream-compaction pipeline on the GPU. This assignment teaches memory coalescing,
            bank-conflict avoidance, and hierarchical parallelism. You will implement both exclusive and inclusive
            scans, then use them to compact an array (remove zeros or inactives) using scatter. This directly maps
            to building and maintaining sparse activation vectors and CSR row pointers.
        </p>

        <h3>3. Sparse Matrix-Vector Multiply (CSR + COO SpMV)</h3>
        <p>
            You will implement both COO and CSR SpMV kernels and compare their performance, memory access patterns,
            divergence behavior, and atomic usage. This includes designing different thread decomposition strategies
            (thread-per-row, warp-per-row, thread-per-nonzero). You will also implement a basic COO→CSR conversion
            using prefix sums. This assignment provides a basis for implementing sparse neural network
            forward/backward passes.
        </p>

        <h3>4. Persistent Kernels + Global Work Queues</h3>
        <p>
            You will build a persistent-threads kernel that stays resident on the GPU and continuously pulls work
            from a global queue implemented with atomics. This teaches you how to avoid kernel launch overhead and
            handle asynchronous or streaming workloads. You'll implement task distribution (pop), task creation
            (push), and termination detection safely under concurrency. This assignment mirrors what real
            sparse/dynamic NN frameworks must do to process many tiny updates in real time. We could also optionally
            use this as another chance to get more profiling experience.
        </p>
    </div>

    <h2>Other Possible Assignments</h2>

    <div class="section">
        <h3>Dynamic Graph Update + Periodic CSR Rebuild</h3>
        <p>
            You implement a dynamic adjacency structure where edges can be appended atomically (COO-style), then
            periodically rebuild a CSR representation using prefix scans and compaction. This teaches how to manage
            variable-length lists, amortize dynamic updates, and maintain consistency between representations. You
            will design a chunked, GPU-friendly memory structure for insertions, then build a fast, parallel CSR
            rebuild pipeline. This assignment is the bridge between static sparse operators and true
            evolving-connectivity neural networks.
        </p>

        <h3>Parallel BFS / Frontier Expansion</h3>
        <p>
            You will implement a GPU BFS that maintains a frontier queue and processes only active nodes on each
            iteration. This teaches workload compaction, atomic insertion into a next-level frontier, and handling
            irregular per-node degrees. You will optimize your BFS using warp-cooperative neighbor processing and
            compaction to eliminate empty work. This assignment provides deep intuition about divergence, load
            balance, and irregular memory access patterns—core challenges in sparse and dynamic neural network
            architectures.
        </p>
    </div>

    <h2>Potential Reading List</h2>

    <div class="section">
        <h3>GPU Architecture and Programming</h3>
        <ul>
            <li>
                <strong>Parallel Processing Fundamentals</strong><br>
                Parallel Processing (2nd edition textbook)
            </li>
            <li>
                <strong>Blackwell GPU Architecture Characterization</strong><br>
                <a href="https://arxiv.org/abs/2507.10789v1">arXiv:2507.10789v1</a><br>
                Microbenchmarking study revealing architectural characteristics of the Blackwell platform
            </li>
            <li>
                <strong>CUDA Core Microbenchmarking</strong><br>
                <a href="https://dl.acm.org/doi/10.1145/3725843.3756041">ACM DL</a><br>
                Comparison point for understanding CUDA core architecture versus Blackwell
            </li>
            <li>
                <strong>AMD CDNA3 Architecture Reference</strong><br>
                <a href="https://www.amd.com/content/dam/amd/en/documents/instinct-tech-docs/instruction-set-architectures/amd-instinct-mi300-cdna3-instruction-set-architecture.pdf">MI300A Instruction Set Architecture</a><br>
                Reference documentation for comparison with NVIDIA architectures
            </li>
        </ul>

        <h3>Sparse Neural Networks and Efficiency</h3>
        <ul>
            <li>
                <strong>The Big World Hypothesis</strong><br>
                <a href="https://openreview.net/pdf?id=Sv7DazuCn8">OpenReview</a><br>
                Motivates the need for sparse and adaptive approaches in large-scale learning
            </li>
            <li>
                <strong>Gale, T., Elsen, E., and Hooker, S. (2019)</strong><br>
                <em>Sparsity in Deep Learning: Pruning and Growth for Efficient Inference and Training in Neural Networks</em><br>
                <a href="https://arxiv.org/pdf/2102.00554">arXiv:2102.00554</a><br>
                Comprehensive survey of sparsity techniques and their importance in deep learning
            </li>
            <li>
                <strong>[PLACEHOLDER] Network Architecture and Learning Efficiency</strong><br>
                Paper demonstrating the importance of network architecture on performance, learning efficiency, and generalization
            </li>
        </ul>

        <h3>Streaming and Continual Learning</h3>
        <ul>
            <li>
                <strong>Streaming Reinforcement Learning Finally Works</strong><br>
                <a href="https://arxiv.org/abs/2410.14606">arXiv:2410.14606</a><br>
                Requirements for successful streaming algorithms in RL (step-size adaptation, normalization, etc.)
            </li>
            <li>
                <strong>[PLACEHOLDER] Online Network Structure Learning</strong><br>
                Paper on learning network structure online during training<br>
                <em>Candidates:</em>
                <ul style="margin-top: 5px;">
                    <li><a href="https://arxiv.org/pdf/1607.01097">AdaNet: Adaptive Structural Learning of Artificial Neural Networks</a></li>
                    <li><a href="https://link.springer.com/chapter/10.1007/978-3-032-05461-6_13">Efficient Architecture Search for Continual Learning</a></li>
                    <li>Neuron-Level Architecture Search for Efficient Model Design</li>
                </ul>
            </li>
        </ul>

        <p>
            <em>Note:</em> Continuous learning ideas might be a much better focus for paper reading in the course.
            Implementing the system on an actual GPU would be an easier engineering problem and additional papers
            can be sourced as required.
        </p>
    </div>
    <footer style="margin-top: 50px; padding-top: 20px; border-top: 1px solid #ccc; color: #666; font-size: 0.9em;">

        <p>Course webpage maintained by the Department of Computing Science, University of Alberta</p>
        <p>For questions or concerns, please contact the course instructor.</p>
    </footer>
</body>
</html>
